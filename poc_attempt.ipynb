{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/60, manager_return=0.052, epsM=0.99, epsW=0.99\n",
      "Episode 2/60, manager_return=0.000, epsM=0.99, epsW=0.99\n",
      "Episode 3/60, manager_return=0.006, epsM=0.99, epsW=0.99\n",
      "Episode 4/60, manager_return=0.008, epsM=0.98, epsW=0.98\n",
      "Episode 5/60, manager_return=0.015, epsM=0.98, epsW=0.98\n",
      "Episode 6/60, manager_return=0.000, epsM=0.97, epsW=0.97\n",
      "Episode 7/60, manager_return=0.000, epsM=0.97, epsW=0.97\n",
      "Episode 8/60, manager_return=0.000, epsM=0.96, epsW=0.96\n",
      "Episode 9/60, manager_return=0.000, epsM=0.96, epsW=0.96\n",
      "Episode 10/60, manager_return=0.000, epsM=0.95, epsW=0.95\n",
      "Episode 11/60, manager_return=0.004, epsM=0.95, epsW=0.95\n",
      "Episode 12/60, manager_return=0.004, epsM=0.94, epsW=0.94\n",
      "Episode 13/60, manager_return=0.010, epsM=0.94, epsW=0.94\n",
      "Episode 14/60, manager_return=0.000, epsM=0.93, epsW=0.93\n",
      "Episode 15/60, manager_return=0.004, epsM=0.93, epsW=0.93\n",
      "Episode 16/60, manager_return=0.015, epsM=0.92, epsW=0.92\n",
      "Episode 17/60, manager_return=0.004, epsM=0.92, epsW=0.92\n",
      "Episode 18/60, manager_return=0.000, epsM=0.91, epsW=0.91\n",
      "Episode 19/60, manager_return=0.027, epsM=0.91, epsW=0.91\n",
      "Episode 20/60, manager_return=0.000, epsM=0.90, epsW=0.90\n",
      "Episode 21/60, manager_return=0.000, epsM=0.90, epsW=0.90\n",
      "Episode 22/60, manager_return=0.004, epsM=0.90, epsW=0.90\n",
      "Episode 23/60, manager_return=0.010, epsM=0.89, epsW=0.89\n",
      "Episode 24/60, manager_return=0.008, epsM=0.89, epsW=0.89\n",
      "Episode 25/60, manager_return=0.004, epsM=0.88, epsW=0.88\n",
      "Episode 26/60, manager_return=0.000, epsM=0.88, epsW=0.88\n",
      "Episode 27/60, manager_return=0.010, epsM=0.87, epsW=0.87\n",
      "Episode 28/60, manager_return=0.015, epsM=0.87, epsW=0.87\n",
      "Episode 29/60, manager_return=0.000, epsM=0.86, epsW=0.86\n",
      "Episode 30/60, manager_return=0.004, epsM=0.86, epsW=0.86\n",
      "Episode 31/60, manager_return=0.013, epsM=0.86, epsW=0.86\n",
      "Episode 32/60, manager_return=0.004, epsM=0.85, epsW=0.85\n",
      "Episode 33/60, manager_return=0.000, epsM=0.85, epsW=0.85\n",
      "Episode 34/60, manager_return=0.038, epsM=0.84, epsW=0.84\n",
      "Episode 35/60, manager_return=0.000, epsM=0.84, epsW=0.84\n",
      "Episode 36/60, manager_return=0.008, epsM=0.83, epsW=0.83\n",
      "Episode 37/60, manager_return=0.013, epsM=0.83, epsW=0.83\n",
      "Episode 38/60, manager_return=0.004, epsM=0.83, epsW=0.83\n",
      "Episode 39/60, manager_return=0.006, epsM=0.82, epsW=0.82\n",
      "Episode 40/60, manager_return=0.006, epsM=0.82, epsW=0.82\n",
      "Episode 41/60, manager_return=0.015, epsM=0.81, epsW=0.81\n",
      "Episode 42/60, manager_return=0.000, epsM=0.81, epsW=0.81\n",
      "Episode 43/60, manager_return=0.023, epsM=0.81, epsW=0.81\n",
      "Episode 44/60, manager_return=0.000, epsM=0.80, epsW=0.80\n",
      "Episode 45/60, manager_return=0.000, epsM=0.80, epsW=0.80\n",
      "Episode 46/60, manager_return=0.004, epsM=0.79, epsW=0.79\n",
      "Episode 47/60, manager_return=0.008, epsM=0.79, epsW=0.79\n",
      "Episode 48/60, manager_return=0.006, epsM=0.79, epsW=0.79\n",
      "Episode 49/60, manager_return=0.025, epsM=0.78, epsW=0.78\n",
      "Episode 50/60, manager_return=0.000, epsM=0.78, epsW=0.78\n",
      "Episode 51/60, manager_return=0.052, epsM=0.77, epsW=0.77\n",
      "Episode 52/60, manager_return=0.010, epsM=0.77, epsW=0.77\n",
      "Episode 53/60, manager_return=0.004, epsM=0.77, epsW=0.77\n",
      "Episode 54/60, manager_return=0.000, epsM=0.76, epsW=0.76\n",
      "Episode 55/60, manager_return=0.004, epsM=0.76, epsW=0.76\n",
      "Episode 56/60, manager_return=0.006, epsM=0.76, epsW=0.76\n",
      "Episode 57/60, manager_return=0.004, epsM=0.75, epsW=0.75\n",
      "Episode 58/60, manager_return=0.013, epsM=0.75, epsW=0.75\n",
      "Episode 59/60, manager_return=0.017, epsM=0.74, epsW=0.74\n",
      "Episode 60/60, manager_return=0.004, epsM=0.74, epsW=0.74\n",
      "Feudal training complete.\n",
      "Creating final 3D sculpting demo...\n",
      "Demo run saved to feudal_3d_sculpt_demo.gif\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Feudal RL: 3D Sculpting with a Beautiful 3D Voxel Visualization\n",
    "---------------------------------------------------------------\n",
    "We have:\n",
    " - A 3D NxNxN environment with an initial stock (True => present) \n",
    "   and a spherical \"shape\" to protect (True => shape).\n",
    " - The manager picks subgoals in [0..N^3 -1] (the subgoal is a coordinate in [0..N-1]^3).\n",
    " - The worker picks discrete moves among 6 directions (±x, ±y, ±z).\n",
    " - We do difference-based manager rewards based on fraction of outside removed.\n",
    " - We produce a \"pretty\" 3D voxel visualization for each step, storing frames and \n",
    "   compiling them into a GIF. The shape is displayed in a distinctive color, \n",
    "   the stock in partial alpha, and the router is shown as a bright 3D marker (voxel).\n",
    "Usage:\n",
    "  python feudal_3d_pretty.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import io\n",
    "import imageio\n",
    "\n",
    "# -------------------------\n",
    "# 1) ENVIRONMENT\n",
    "# -------------------------\n",
    "class Sculpt3DEnv:\n",
    "    \"\"\"\n",
    "    3D voxel-based sculpting environment:\n",
    "     - NxNxN stock\n",
    "     - NxNxN shape (protected)\n",
    "     - The manager sets subgoals in discrete coords\n",
    "     - The worker carves 1-step moves among 6 directions\n",
    "     - If shape is cut or OOB => done\n",
    "     - Worker reward = small positive for removing outside stock\n",
    "     - Manager reward = difference-based fraction_outside_removed\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=8, max_steps=200, manager_update_freq=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.manager_update_freq = manager_update_freq\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # stock True => material present\n",
    "        self.stock = np.ones((self.grid_size, self.grid_size, self.grid_size), dtype=bool)\n",
    "        # shape True => protected region\n",
    "        self.shape = np.zeros((self.grid_size, self.grid_size, self.grid_size), dtype=bool)\n",
    "        cx,cy,cz = self.grid_size//2, self.grid_size//2, self.grid_size//2\n",
    "        r = self.grid_size//3\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                for z in range(self.grid_size):\n",
    "                    dx,dy,dz = x-cx,y-cy,z-cz\n",
    "                    dist = dx*dx+dy*dy+dz*dz\n",
    "                    if dist<=r*r:\n",
    "                        self.shape[x,y,z] = True\n",
    "\n",
    "        self.router_pos = np.array([0,0,0], dtype=int)\n",
    "        self.steps_taken = 0\n",
    "        self.done = False\n",
    "        self.subgoal = np.array([cx,cy,cz], dtype=int)\n",
    "        return self._get_manager_obs(), self._get_worker_obs()\n",
    "\n",
    "    def set_subgoal(self, coord):\n",
    "        coord = np.clip(coord,0,self.grid_size-1)\n",
    "        self.subgoal = coord\n",
    "\n",
    "    def worker_step(self, action):\n",
    "        \"\"\"\n",
    "        6 discrete moves:\n",
    "         0=+x,1=-x,2=+y,3=-y,4=+z,5=-z\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self._get_worker_obs(),0.0,True\n",
    "        move = np.array([0,0,0], dtype=int)\n",
    "        if action==0: move=np.array([1,0,0])\n",
    "        elif action==1:move=np.array([-1,0,0])\n",
    "        elif action==2:move=np.array([0,1,0])\n",
    "        elif action==3:move=np.array([0,-1,0])\n",
    "        elif action==4:move=np.array([0,0,1])\n",
    "        elif action==5:move=np.array([0,0,-1])\n",
    "\n",
    "        oldp = self.router_pos.copy()\n",
    "        newp = oldp+move\n",
    "        reward=0.0\n",
    "\n",
    "        # check OOB\n",
    "        if not np.all((newp>=0)&(newp<self.grid_size)):\n",
    "            reward-=5.0\n",
    "            self.done=True\n",
    "        else:\n",
    "            path = [oldp, newp]\n",
    "            for (vx,vy,vz) in path:\n",
    "                if self.shape[vx,vy,vz]:\n",
    "                    reward-=5.0\n",
    "                    self.done=True\n",
    "                    break\n",
    "                if self.stock[vx,vy,vz]:\n",
    "                    self.stock[vx,vy,vz]=False\n",
    "                    reward+=1.0\n",
    "            if not self.done:\n",
    "                self.router_pos=newp\n",
    "\n",
    "        self.steps_taken+=1\n",
    "        if self.steps_taken>=self.max_steps:\n",
    "            self.done=True\n",
    "        return self._get_worker_obs(),reward,self.done\n",
    "\n",
    "    def manager_reward(self):\n",
    "        outside_mask = (self.shape==False)\n",
    "        outside_total = np.sum(outside_mask)\n",
    "        outside_removed = outside_total - np.sum(self.stock[outside_mask])\n",
    "        frac_removed = outside_removed/(outside_total+1e-8)\n",
    "        return frac_removed\n",
    "\n",
    "    def manager_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_manager_obs(self):\n",
    "        outside_mask = (self.shape==False)\n",
    "        outside_total = np.sum(outside_mask)\n",
    "        outside_removed = outside_total - np.sum(self.stock[outside_mask])\n",
    "        frac_removed = outside_removed/(outside_total+1e-8)\n",
    "        rx,ry,rz = self.router_pos\n",
    "        return np.array([frac_removed, rx, ry, rz], dtype=float)\n",
    "\n",
    "    def _get_worker_obs(self):\n",
    "        rx,ry,rz = self.router_pos\n",
    "        sx,sy,sz = self.subgoal\n",
    "        return np.array([rx,ry,rz,sx,sy,sz], dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# 2) NETWORKS\n",
    "# -------------------------\n",
    "class ManagerNet(nn.Module):\n",
    "    def __init__(self, input_dim, grid_size=8, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.output_dim = grid_size**3\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class WorkerNet(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, n_actions=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------\n",
    "# 3) REPLAY BUFFERS\n",
    "# -------------------------\n",
    "class ManagerReplayBuffer:\n",
    "    def __init__(self, cap=5000):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "    def push(self,s,a,r,s_next,d):\n",
    "        self.buffer.append((s,a,r,s_next,d))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer,batch_size)\n",
    "        s,a,r,ns,d = zip(*batch)\n",
    "        return np.array(s), np.array(a), np.array(r), np.array(ns), np.array(d)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class WorkerReplayBuffer:\n",
    "    def __init__(self, cap=5000):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "    def push(self,s,a,r,s_next,d):\n",
    "        self.buffer.append((s,a,r,s_next,d))\n",
    "    def sample(self,batch_size):\n",
    "        batch = random.sample(self.buffer,batch_size)\n",
    "        s,a,r,ns,d = zip(*batch)\n",
    "        return np.array(s), np.array(a), np.array(r), np.array(ns), np.array(d)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# -------------------------\n",
    "# 4) TRAINING\n",
    "# -------------------------\n",
    "def feudal_train(env,\n",
    "                 manager_episodes=100,\n",
    "                 batch_size=32,\n",
    "                 manager_lr=1e-3,\n",
    "                 worker_lr=1e-3,\n",
    "                 gamma_manager=0.9,\n",
    "                 gamma_worker=0.9,\n",
    "                 manager_eps_start=1.0,\n",
    "                 worker_eps_start=1.0,\n",
    "                 eps_min=0.1,\n",
    "                 eps_decay=0.995):\n",
    "    device=\"cpu\"\n",
    "    manager_obs_dim = len(env._get_manager_obs())\n",
    "    worker_obs_dim  = len(env._get_worker_obs())\n",
    "    manager_net = ManagerNet(manager_obs_dim, grid_size=env.grid_size).to(device)\n",
    "    worker_net  = WorkerNet(worker_obs_dim, 128, 6).to(device)\n",
    "\n",
    "    manager_opt=optim.Adam(manager_net.parameters(),lr=manager_lr)\n",
    "    worker_opt= optim.Adam(worker_net.parameters(), lr=worker_lr)\n",
    "\n",
    "    manager_replay=ManagerReplayBuffer(5000)\n",
    "    worker_replay= WorkerReplayBuffer(5000)\n",
    "\n",
    "    manager_eps = manager_eps_start\n",
    "    worker_eps  = worker_eps_start\n",
    "\n",
    "    for ep in range(manager_episodes):\n",
    "        m_obs, w_obs = env.reset()\n",
    "        done=False\n",
    "        manager_return=0.0\n",
    "        while not done:\n",
    "            # manager picks subgoal\n",
    "            manager_s=m_obs\n",
    "            n_subgoals=env.grid_size**3\n",
    "            if random.random()<manager_eps:\n",
    "                subgoal_id = random.randint(0,n_subgoals-1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    inp = torch.FloatTensor(manager_s).unsqueeze(0)\n",
    "                    logits = manager_net(inp)\n",
    "                    subgoal_id=logits.argmax(dim=1).item()\n",
    "            z = subgoal_id%env.grid_size\n",
    "            y = (subgoal_id//env.grid_size)%env.grid_size\n",
    "            x = (subgoal_id//(env.grid_size*env.grid_size))%env.grid_size\n",
    "            env.set_subgoal(np.array([x,y,z]))\n",
    "\n",
    "            old_manager_r=env.manager_reward()\n",
    "            # worker does env.manager_update_freq steps\n",
    "            for _ in range(env.manager_update_freq):\n",
    "                if env.done:\n",
    "                    break\n",
    "                worker_s=w_obs\n",
    "                if random.random()<worker_eps:\n",
    "                    action_worker = random.randint(0,5)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        inp = torch.FloatTensor(worker_s).unsqueeze(0)\n",
    "                        qvals = worker_net(inp)\n",
    "                        action_worker = qvals.argmax(dim=1).item()\n",
    "                w_obs_next, w_r, w_done = env.worker_step(action_worker)\n",
    "                worker_replay.push(worker_s,action_worker,w_r,w_obs_next,w_done)\n",
    "                w_obs=w_obs_next\n",
    "                done=w_done\n",
    "                # update worker\n",
    "                if len(worker_replay)>=batch_size:\n",
    "                    s_arr,a_arr,r_arr,ns_arr,d_arr = worker_replay.sample(batch_size)\n",
    "                    s_t = torch.FloatTensor(s_arr)\n",
    "                    a_t = torch.LongTensor(a_arr)\n",
    "                    r_t = torch.FloatTensor(r_arr)\n",
    "                    ns_t= torch.FloatTensor(ns_arr)\n",
    "                    d_t = torch.BoolTensor(d_arr)\n",
    "\n",
    "                    q_vals = worker_net(s_t)\n",
    "                    q_s_a = q_vals.gather(1, a_t.unsqueeze(1)).squeeze(1)\n",
    "                    with torch.no_grad():\n",
    "                        q_next = worker_net(ns_t)\n",
    "                        max_q_next,_=torch.max(q_next,dim=1)\n",
    "                        max_q_next[d_t]=0.0\n",
    "                    target = r_t + gamma_worker*max_q_next\n",
    "                    loss = nn.MSELoss()(q_s_a,target)\n",
    "                    worker_opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    worker_opt.step()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            m_obs_next = env._get_manager_obs()\n",
    "            new_manager_r= env.manager_reward()\n",
    "            delta_r = new_manager_r - old_manager_r\n",
    "            manager_return+=delta_r\n",
    "            manager_done=env.manager_done()\n",
    "            manager_replay.push(manager_s, subgoal_id, delta_r, m_obs_next, manager_done)\n",
    "            m_obs=m_obs_next\n",
    "            done=manager_done\n",
    "            # manager update\n",
    "            if len(manager_replay)>=batch_size:\n",
    "                s_arr,a_arr,r_arr,ns_arr,d_arr = manager_replay.sample(batch_size)\n",
    "                s_t = torch.FloatTensor(s_arr)\n",
    "                a_t = torch.LongTensor(a_arr)\n",
    "                r_t = torch.FloatTensor(r_arr)\n",
    "                ns_t= torch.FloatTensor(ns_arr)\n",
    "                d_t = torch.BoolTensor(d_arr)\n",
    "\n",
    "                logits = manager_net(s_t)\n",
    "                q_s_a = logits.gather(1, a_t.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    logits_next = manager_net(ns_t)\n",
    "                    max_q_next,_=torch.max(logits_next,dim=1)\n",
    "                    max_q_next[d_t]=0.0\n",
    "                target = r_t + gamma_manager*max_q_next\n",
    "                loss_m = nn.MSELoss()(q_s_a,target)\n",
    "                manager_opt.zero_grad()\n",
    "                loss_m.backward()\n",
    "                manager_opt.step()\n",
    "\n",
    "        manager_eps = max(eps_min, manager_eps*eps_decay)\n",
    "        worker_eps  = max(eps_min, worker_eps*eps_decay)\n",
    "        print(f\"Episode {ep+1}/{manager_episodes}, manager_return={manager_return:.3f}, epsM={manager_eps:.2f}, epsW={worker_eps:.2f}\")\n",
    "\n",
    "    return manager_net, worker_net\n",
    "\n",
    "# -------------------------\n",
    "# 5) 3D Voxel Visualization\n",
    "# -------------------------\n",
    "def create_3d_demo_gif(manager_net, worker_net, env, output_gif=\"feudal_3d_sculpt_demo.gif\"):\n",
    "    \"\"\"\n",
    "    Runs a final episode with manager & worker in greedy mode, capturing a \n",
    "    PRETTY 3D voxel rendering for each step. We store frames and compile into a GIF.\n",
    "    We'll use partial alpha for stock, bright color for shape, a different color for router.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    manager_eps=0.0\n",
    "    worker_eps=0.0\n",
    "    m_obs, w_obs = env.reset()\n",
    "    done=False\n",
    "\n",
    "    while not done:\n",
    "        # manager picks subgoal greedily\n",
    "        manager_s = m_obs\n",
    "        with torch.no_grad():\n",
    "            inp = torch.FloatTensor(manager_s).unsqueeze(0)\n",
    "            logits = manager_net(inp)\n",
    "            subgoal_id = logits.argmax(dim=1).item()\n",
    "\n",
    "        z = subgoal_id % env.grid_size\n",
    "        y = (subgoal_id // env.grid_size) % env.grid_size\n",
    "        x = (subgoal_id // (env.grid_size*env.grid_size)) % env.grid_size\n",
    "        env.set_subgoal(np.array([x,y,z]))\n",
    "\n",
    "        # up to env.manager_update_freq worker steps\n",
    "        old_mgr_r = env.manager_reward()\n",
    "        for _ in range(env.manager_update_freq):\n",
    "            if env.done:\n",
    "                break\n",
    "\n",
    "            # worker picks move greedily\n",
    "            worker_s = w_obs\n",
    "            with torch.no_grad():\n",
    "                inp = torch.FloatTensor(worker_s).unsqueeze(0)\n",
    "                qvals = worker_net(inp)\n",
    "                act = qvals.argmax(dim=1).item()\n",
    "\n",
    "            w_obs_next, w_r, w_done = env.worker_step(act)\n",
    "            w_obs = w_obs_next\n",
    "\n",
    "            # gather a pretty 3D voxel snapshot\n",
    "            frames.append(render_3d_voxel(env))\n",
    "\n",
    "            if w_done:\n",
    "                break\n",
    "\n",
    "        m_obs_next = env._get_manager_obs()\n",
    "        done=env.manager_done()\n",
    "        m_obs=m_obs_next\n",
    "\n",
    "    # compile frames into a GIF\n",
    "    imageio.mimsave(output_gif, frames, fps=2)\n",
    "    print(f\"Demo run saved to {output_gif}\")\n",
    "\n",
    "def render_3d_voxel(env):\n",
    "    \"\"\"\n",
    "    Return a PIL image with a 3D voxel rendering:\n",
    "     - shape in bright color\n",
    "     - stock in partial alpha\n",
    "     - router as a single bright voxel\n",
    "    We'll do an ax.voxels(...) approach.\n",
    "    We'll color shape vs. outside stock differently, plus an overlay for router.\n",
    "    \"\"\"\n",
    "    from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    n = env.grid_size\n",
    "\n",
    "    # We'll create an RGBA array for each voxel. shape => bright color, stock => partial color, removed => alpha=0\n",
    "    stock_bool = env.stock  # NxNxN\n",
    "    shape_bool = env.shape\n",
    "\n",
    "    # create a boolean 3D array for \"visible\" => shape or stock\n",
    "    # We'll differentiate them by color\n",
    "    # For shape: if shape[x,y,z] is True and stock[x,y,z] is True => shape voxel\n",
    "    # For outside: if shape[x,y,z]==False and stock[x,y,z]==True => outside voxel\n",
    "    # Everything else => alpha=0\n",
    "    # Then we'll do a separate single voxel for the router.\n",
    "\n",
    "    data_color = np.zeros((n,n,n,4), dtype=float)\n",
    "    # shape => bright red or so\n",
    "    # outside => gray with partial alpha\n",
    "    # shape removed => alpha=0\n",
    "    # outside removed => alpha=0\n",
    "\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            for z in range(n):\n",
    "                if shape_bool[x,y,z] and stock_bool[x,y,z]:\n",
    "                    # shape voxel => color = [1,0,0,1]\n",
    "                    data_color[x,y,z] = [1.0, 0.2, 0.2, 1.0]\n",
    "                elif (not shape_bool[x,y,z]) and stock_bool[x,y,z]:\n",
    "                    # outside => partial alpha grey\n",
    "                    data_color[x,y,z] = [0.5,0.5,0.5,0.5]\n",
    "                else:\n",
    "                    data_color[x,y,z] = [1.0,1.0,1.0,0.0]\n",
    "\n",
    "    # router => single voxel with color = [0,1,0,1]\n",
    "    rx,ry,rz = env.router_pos\n",
    "    data_color[rx,ry,rz] = [0.0, 1.0, 0.0, 1.0]\n",
    "\n",
    "    # ax.voxels wants a boolean 3D array for \"filled\", plus facecolors\n",
    "    filled = (data_color[:,:,:,3]>0.0)  # anything with alpha>0\n",
    "    ax.voxels(filled, facecolors=data_color, edgecolor=None)\n",
    "\n",
    "    ax.set_xlim(0,n)\n",
    "    ax.set_ylim(0,n)\n",
    "    ax.set_zlim(0,n)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_title(\"Feudal 3D Sculpt\")\n",
    "\n",
    "    # convert figure to PIL image\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    canvas.draw()\n",
    "    width, height = fig.canvas.get_width_height()\n",
    "    buf = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    buf = buf.reshape(height, width, 3)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return buf\n",
    "\n",
    "# -------------------------\n",
    "# 6) MAIN\n",
    "# -------------------------\n",
    "def main():\n",
    "    env = Sculpt3DEnv(grid_size=8, max_steps=60, manager_update_freq=5)\n",
    "    manager_net, worker_net = feudal_train(env, manager_episodes=40)\n",
    "    print(\"Creating final 3D sculpting demo with pretty voxel rendering ...\")\n",
    "    create_3d_demo_gif(manager_net, worker_net, env, \"feudal_3d_sculpt_demo.gif\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/braeden/Development/Yale/cpsc776/cpsc776_demo/.venv/lib/python3.9/site-packages (from imageio) (1.26.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/braeden/Development/Yale/cpsc776/cpsc776_demo/.venv/lib/python3.9/site-packages (from imageio) (11.0.0)\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.37.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install imageio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
