{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-Based Architecture\n",
    "\n",
    "## Input\n",
    "\n",
    "3 Channels --> stock, shape, router  \n",
    "We treat each 10×10 grid (stock, shape, and router location) as a separate channel, so the CNN can learn spatial relationships among material, protected shape, and the agent’s position.  \n",
    "Concatenate the channels along the image dimension (like RGB) so we have 1. stock channel (1 where uncut stock remains) 2. shape channel (1 for protected shape, 0 otherwise) and 3. router channel: 1 at the routers grid cell, 0 otherwise.\n",
    "\n",
    "## Convolutional layer 1\n",
    "\n",
    "32 filters, kernel size 3x3, stride = 1, ReLU activation  \n",
    "Why 32 filters? Enough capacity to capture local patterns (edges, corners, small features) without being too large for a small 10×10 input.  \n",
    "Why a 3×3 kernel? This is a classic, effective “local receptive field” size that balances fine detail with efficient training.  \n",
    "\n",
    "## Convolutional Layer 2\n",
    "64 filters, kernel size 3x3, stride = 1, ReLU activation\n",
    "Why another layer? Stacking conv layers lets the network learn higher-order features (combinations of the lower-layer edge/texture patterns) – essential for distinguishing shape boundaries vs. outside stock.  \n",
    "Why 64 filters? Doubling filters in the second layer is a common practice, giving more representational power for more complex patterns.  \n",
    "\n",
    "## Pooling Layer\n",
    "2x2 max pooling  \n",
    "reduces spatial resolution by half (10x10 to 5x5), lowers compute cost and increasing receptive field (avoid overfitting on small details)  \n",
    "\n",
    "## Flatten\n",
    "\n",
    "Why? After extracting spatial features, flattening converts the feature map into a 1D vector for a fully connected layer. This merges all local feature activations into a representation for decision-making.  \n",
    "\n",
    "## Fully connected layer\n",
    "128 units, relu activation  \n",
    "Why 128? It’s enough capacity to combine and interpret the learned spatial features without being excessively large for a 10×10 grid.  \n",
    "\n",
    "## Output Layer\n",
    "Config: size = number of actions  \n",
    "Why? In a DQN, this final layer directly outputs the Q-values for each discrete action. We only need as many outputs as there are possible moves.  \n",
    "\n",
    "By combining two convolutional layers (each capturing progressively higher-level spatial features) with a final dense layer (for integrating those features into action values), we get a compact but effective architecture. The small 10×10 input size means a deep or wider network might overfit or be computationally wasteful, so 2 conv layers + 1 dense layer is a balanced choice for this milling task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 44.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: six>1.9 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard) (1.15.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[K     |████████████████████████████████| 417 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /Users/braeden/Library/Python/3.9/lib/python/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard) (58.0.4)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 45.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/braeden/Library/Python/3.9/lib/python/site-packages (from tensorboard) (24.0)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "\u001b[K     |████████████████████████████████| 277 kB 42.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /Users/braeden/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/braeden/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/braeden/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.2.1 grpcio-1.71.0 markdown-3.7 protobuf-6.30.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import display, Image\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 3D ENVIRONMENT DEFINITION\n",
    "# ----------------------------\n",
    "\n",
    "class Milling3DEnv:\n",
    "    \"\"\"\n",
    "    A 3D milling environment on an N x N x N grid:\n",
    "      - stock[x,y,z] = 1 means material present, 0 means removed\n",
    "      - shape[x,y,z] = 1 means protected shape\n",
    "      - router_pos = (rx, ry, rz) within [0, N-1]\n",
    "      - 6 discrete actions: move ±1 in x, y, or z\n",
    "      - always-on cutter\n",
    "      - random sphere in the center for the shape\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size=8,\n",
    "        max_steps=200,\n",
    "        reward_outside_cut=1.0,\n",
    "        penalty_step=0.1,\n",
    "        penalty_cut_shape=10.0,\n",
    "        success_bonus=30.0,\n",
    "        min_radius=2,\n",
    "        max_radius=3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param grid_size: size of the 3D grid\n",
    "        :param max_steps: max steps before termination\n",
    "        :param reward_outside_cut: reward for removing outside stock\n",
    "        :param penalty_step: penalty each step\n",
    "        :param penalty_cut_shape: penalty if we cut the shape\n",
    "        :param success_bonus: bonus if all outside stock is removed\n",
    "        :param min_radius, max_radius: random sphere radius range\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Reward parameters\n",
    "        self.reward_outside_cut = reward_outside_cut\n",
    "        self.penalty_step = penalty_step\n",
    "        self.penalty_cut_shape = penalty_cut_shape\n",
    "        self.success_bonus = success_bonus\n",
    "        \n",
    "        self.min_radius = min_radius\n",
    "        self.max_radius = max_radius\n",
    "        \n",
    "        # 6 actions: 0=+x,1=-x,2=+y,3=-y,4=+z,5=-z\n",
    "        self.action_space = [0,1,2,3,4,5]\n",
    "        \n",
    "        # Will be populated on reset\n",
    "        self.stock = None\n",
    "        self.shape = None\n",
    "        self.router_pos = None\n",
    "        self.steps_taken = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" Initialize the 3D grid and place a random sphere in the center. \"\"\"\n",
    "        self.stock = np.ones((self.grid_size, self.grid_size, self.grid_size), dtype=int)\n",
    "        self.shape = np.zeros((self.grid_size, self.grid_size, self.grid_size), dtype=int)\n",
    "        \n",
    "        cx = self.grid_size // 2\n",
    "        cy = self.grid_size // 2\n",
    "        cz = self.grid_size // 2\n",
    "        \n",
    "        radius = np.random.randint(self.min_radius, self.max_radius+1)\n",
    "        \n",
    "        # Mark shape = 1 for sphere region\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                for z in range(self.grid_size):\n",
    "                    dist_sq = (x-cx)**2 + (y-cy)**2 + (z-cz)**2\n",
    "                    if dist_sq <= radius**2:\n",
    "                        self.shape[x,y,z] = 1\n",
    "        \n",
    "        # Place router at (0,0,0) for simplicity\n",
    "        self.router_pos = np.array([0,0,0], dtype=int)\n",
    "        self.steps_taken = 0\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action in {0,1,2,3,4,5} => ±x, ±y, ±z\n",
    "        router moves 1 voxel in the chosen direction, always-on cutter\n",
    "        \"\"\"\n",
    "        if action == 0:  # +x\n",
    "            self.router_pos[0] += 1\n",
    "        elif action == 1:  # -x\n",
    "            self.router_pos[0] -= 1\n",
    "        elif action == 2:  # +y\n",
    "            self.router_pos[1] += 1\n",
    "        elif action == 3:  # -y\n",
    "            self.router_pos[1] -= 1\n",
    "        elif action == 4:  # +z\n",
    "            self.router_pos[2] += 1\n",
    "        elif action == 5:  # -z\n",
    "            self.router_pos[2] -= 1\n",
    "        \n",
    "        # Clamp within [0, grid_size-1]\n",
    "        self.router_pos = np.clip(self.router_pos, 0, self.grid_size-1)\n",
    "        \n",
    "        self.steps_taken += 1\n",
    "        reward = 0.0\n",
    "        \n",
    "        rx, ry, rz = self.router_pos\n",
    "        # If stock present => cut it\n",
    "        if self.stock[rx, ry, rz] == 1:\n",
    "            self.stock[rx, ry, rz] = 0\n",
    "            # Check if it's shape\n",
    "            if self.shape[rx, ry, rz] == 1:\n",
    "                # cut shape => fail\n",
    "                reward -= self.penalty_cut_shape\n",
    "                done = True\n",
    "                return self._get_observation(), reward, done, {}\n",
    "            else:\n",
    "                # outside => positive reward\n",
    "                reward += self.reward_outside_cut\n",
    "        \n",
    "        # step penalty\n",
    "        reward -= self.penalty_step\n",
    "        \n",
    "        # check if outside stock is all removed\n",
    "        outside_mask = (self.shape == 0)  # shape=0 => outside\n",
    "        if np.sum(self.stock[outside_mask]) == 0:\n",
    "            # success\n",
    "            reward += self.success_bonus\n",
    "            done = True\n",
    "        else:\n",
    "            done = (self.steps_taken >= self.max_steps)\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Return a dict with:\n",
    "         - self.stock\n",
    "         - self.shape\n",
    "         - self.router_pos\n",
    "        We'll form 3 channels for a 3D CNN:\n",
    "          channel0 = stock\n",
    "          channel1 = shape\n",
    "          channel2 = router location\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"stock\": self.stock.copy(),\n",
    "            \"shape\": self.shape.copy(),\n",
    "            \"router_pos\": self.router_pos.copy()\n",
    "        }\n",
    "    \n",
    "    def render(self, azim=45, elev=30):\n",
    "        \"\"\"\n",
    "        Create a 3D scatter plot of all voxels:\n",
    "        - Blue: shape=1, stock=1  (uncut shape)\n",
    "        - Red:  shape=1, stock=0  (shape was cut)\n",
    "        - Gray: shape=0, stock=1  (outside stock present)\n",
    "        - White: shape=0, stock=0 (outside removed)\n",
    "        - Black: router\n",
    "        Nothing is fully opaque; alpha < 1.0 so we can see all points simultaneously.\n",
    "        \"\"\"\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        \n",
    "        shape_and_stock = (self.shape == 1) & (self.stock == 1)\n",
    "        shape_cut       = (self.shape == 1) & (self.stock == 0)\n",
    "        outside_stock   = (self.shape == 0) & (self.stock == 1)\n",
    "        outside_cut     = (self.shape == 0) & (self.stock == 0)\n",
    "        \n",
    "        rx, ry, rz = self.router_pos\n",
    "        \n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # 1) Blue: shape & stock\n",
    "        xs, ys, zs = np.where(shape_and_stock)\n",
    "        if len(xs) > 0:\n",
    "            ax.scatter(xs, ys, zs, c='blue', alpha=0.1, s=20, marker='o', depthshade=False, label=\"Shape Uncut\")\n",
    "        \n",
    "        # 2) Red: shape cut\n",
    "        xs, ys, zs = np.where(shape_cut)\n",
    "        if len(xs) > 0:\n",
    "            ax.scatter(xs, ys, zs, c='red', alpha=0.1, s=20, marker='o', depthshade=False, label=\"Shape Cut\")\n",
    "        \n",
    "        # 3) Gray: outside stock\n",
    "        xs, ys, zs = np.where(outside_stock)\n",
    "        if len(xs) > 0:\n",
    "            ax.scatter(xs, ys, zs, c='gray', alpha=0.1, s=20, marker='o', depthshade=False, label=\"Outside Stock\")\n",
    "        \n",
    "        # 4) White: outside removed\n",
    "        xs, ys, zs = np.where(outside_cut)\n",
    "        if len(xs) > 0:\n",
    "            ax.scatter(xs, ys, zs, c='white', alpha=0.1, s=20, marker='o', depthshade=False, label=\"Outside Removed\")\n",
    "        \n",
    "        # 5) Router in black\n",
    "        ax.scatter(rx, ry, rz, c='black', alpha=1.0, s=60, marker='o', depthshade=False, label=\"Router\")\n",
    "        \n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "        ax.set_zlim(0, self.grid_size)\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "        ax.set_title(f\"Steps={self.steps_taken}\")\n",
    "        \n",
    "        # Optional legend (comment out if you don't need it)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        # Convert to an RGBA NumPy array\n",
    "        w, h = fig.canvas.get_width_height()\n",
    "        rgb_buf = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8).reshape((h, w, 3))\n",
    "        \n",
    "        # Make an RGBA array (full 255 alpha channel for the final image)\n",
    "        rgba_img = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "        rgba_img[..., :3] = rgb_buf\n",
    "        rgba_img[..., 3] = 255\n",
    "        \n",
    "        plt.close(fig)\n",
    "        return rgba_img\n",
    "\n",
    "# ----------------------------\n",
    "# 2) 3D CNN ARCHITECTURE\n",
    "# ----------------------------\n",
    "\n",
    "class Milling3DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    3D CNN that processes a (3, D, D, D) input for stock, shape, router\n",
    "    and outputs Q-values for 6 actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=8, n_channels=3, n_actions=6):\n",
    "        super(Milling3DCNN, self).__init__()\n",
    "        \n",
    "        # We'll do two 3D conv layers, then a pooling\n",
    "        # For a small 8x8x8 grid, we can keep it modest\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv3d(n_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2)  # from 8->4 in each dimension\n",
    "        )\n",
    "        \n",
    "        # Then flatten => fully connected\n",
    "        # 32 filters * 4*4*4 = 32*64 = 2048\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(32*(grid_size//2)*(grid_size//2)*(grid_size//2), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [batch_size, 3, D, D, D]\n",
    "        \"\"\"\n",
    "        feats = self.conv_net(x)  # => [batch_size, 32, 4, 4, 4] if D=8\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "        out = self.fc_net(feats)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# 3) REPLAY BUFFER & UTIL\n",
    "# ----------------------------\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=2000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s_arr, a_arr, r_arr, s_next_arr, done_arr = zip(*batch)\n",
    "        return s_arr, a_arr, r_arr, s_next_arr, done_arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def obs_to_3dtensor(obs, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Convert {stock, shape, router_pos} into (3, D, D, D).\n",
    "    \"\"\"\n",
    "    stock = obs[\"stock\"].astype(np.float32)\n",
    "    shape = obs[\"shape\"].astype(np.float32)\n",
    "    router_map = np.zeros_like(stock, dtype=np.float32)\n",
    "    rx, ry, rz = obs[\"router_pos\"]\n",
    "    router_map[rx, ry, rz] = 1.0\n",
    "    \n",
    "    # stack channels\n",
    "    vol_3ch = np.stack([stock, shape, router_map], axis=0)  # (3,D,D,D)\n",
    "    return torch.tensor(vol_3ch, dtype=torch.float32, device=device)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) TRAINING LOOP (DQN)\n",
    "# ----------------------------\n",
    "\n",
    "def train_dqn_3d(\n",
    "    num_episodes=100,\n",
    "    grid_size=8,\n",
    "    max_steps=200,\n",
    "    gamma=0.9,\n",
    "    lr=1e-3,\n",
    "    batch_size=32,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay=0.995,\n",
    "    replay_capacity=5000,\n",
    "    updates_per_episode=10\n",
    "):\n",
    "    env = Milling3DEnv(grid_size=grid_size, max_steps=max_steps)\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    policy_net = Milling3DCNN(grid_size=grid_size, n_channels=3, n_actions=6).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=replay_capacity)\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        state_t = obs_to_3dtensor(obs, device=device)\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        \n",
    "        for step_i in range(env.max_steps):\n",
    "            # Epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(env.action_space)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = policy_net(state_t.unsqueeze(0))  # [1,6]\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "            \n",
    "            obs_next, reward, done, _ = env.step(action)\n",
    "            next_state_t = obs_to_3dtensor(obs_next, device=device)\n",
    "            \n",
    "            # Store CPU arrays in replay\n",
    "            replay_buffer.push((\n",
    "                state_t.cpu().numpy(),\n",
    "                action,\n",
    "                reward,\n",
    "                next_state_t.cpu().numpy(),\n",
    "                done\n",
    "            ))\n",
    "            \n",
    "            state_t = next_state_t\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # After episode, do multiple training updates\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            for _ in range(updates_per_episode):\n",
    "                s_arr, a_arr, r_arr, s_next_arr, done_arr = replay_buffer.sample(batch_size)\n",
    "                \n",
    "                s_batch = torch.tensor(s_arr, dtype=torch.float32, device=device)\n",
    "                a_batch = torch.tensor(a_arr, dtype=torch.long, device=device)\n",
    "                r_batch = torch.tensor(r_arr, dtype=torch.float32, device=device)\n",
    "                s_next_batch = torch.tensor(s_next_arr, dtype=torch.float32, device=device)\n",
    "                done_batch = torch.tensor(done_arr, dtype=torch.bool, device=device)\n",
    "                \n",
    "                # shape of s_batch => [B, 3, D, D, D]\n",
    "                # forward pass\n",
    "                q_values = policy_net(s_batch)\n",
    "                q_chosen = q_values.gather(1, a_batch.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    q_next = policy_net(s_next_batch)\n",
    "                    max_q_next, _ = torch.max(q_next, dim=1)\n",
    "                    max_q_next[done_batch] = 0.0\n",
    "                target = r_batch + gamma * max_q_next\n",
    "                \n",
    "                loss = nn.MSELoss()(q_chosen, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        print(f\"Episode {ep+1}/{num_episodes}, total_reward={total_reward:.2f}, eps={epsilon:.2f}\")\n",
    "    \n",
    "    print(\"3D DQN training complete.\")\n",
    "    return policy_net\n",
    "\n",
    "# ----------------------------\n",
    "# 5) DEMO & GIF CREATION\n",
    "# ----------------------------\n",
    "\n",
    "def create_3d_demo_gif(policy_net, grid_size=8, max_steps=300, output_gif=\"milling_3d_demo.gif\"):\n",
    "    \"\"\"\n",
    "    Use a well-trained policy to run a single episode in 3D,\n",
    "    capturing frames with a fixed camera and a black path\n",
    "    tracing the router's movement.\n",
    "    \"\"\"\n",
    "    env = Milling3DEnv(grid_size=grid_size, max_steps=max_steps)\n",
    "    obs = env.reset()\n",
    "    \n",
    "    frames = []\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    for step_i in range(env.max_steps):\n",
    "        # Render the current frame (no camera rotation)\n",
    "        rgba = env.render()  \n",
    "        frames.append(rgba)\n",
    "        \n",
    "        # Choose action from the trained policy\n",
    "        state_t = obs_to_3dtensor(obs, device=device)\n",
    "        with torch.no_grad():\n",
    "            q_vals = policy_net(state_t.unsqueeze(0))  # shape [1,6]\n",
    "            action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        obs_next, reward, done, _ = env.step(action)\n",
    "        obs = obs_next\n",
    "        \n",
    "        if done:\n",
    "            # Capture one last frame after termination\n",
    "            frames.append(env.render())\n",
    "            break\n",
    "    \n",
    "    imageio.mimsave(output_gif, frames, fps=2)\n",
    "    print(f\"Demo GIF saved to {output_gif}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10, total_reward=8.30, eps=0.98\n",
      "Episode 2/10, total_reward=17.80, eps=0.96\n",
      "Episode 3/10, total_reward=-1.20, eps=0.94\n",
      "Episode 4/10, total_reward=7.90, eps=0.92\n",
      "Episode 5/10, total_reward=-3.10, eps=0.90\n",
      "Episode 6/10, total_reward=53.00, eps=0.89\n",
      "Episode 7/10, total_reward=-2.90, eps=0.87\n",
      "Episode 8/10, total_reward=16.10, eps=0.85\n",
      "Episode 9/10, total_reward=33.40, eps=0.83\n",
      "Episode 10/10, total_reward=25.00, eps=0.82\n",
      "3D DQN training complete.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6) EXAMPLE USAGE\n",
    "# ----------------------------\n",
    "model = train_dqn_3d(\n",
    "    num_episodes=1000,    # Increase for better policies\n",
    "    grid_size=8,\n",
    "    max_steps=200,\n",
    "    gamma=0.9,\n",
    "    lr=1e-3,\n",
    "    batch_size=16,      # smaller batch if your CPU is slow\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay=0.98,\n",
    "    replay_capacity=5000,\n",
    "    updates_per_episode=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/9hmy_qv946z1t5m7z35vf6600000gn/T/ipykernel_43233/3480441810.py:221: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  rgb_buf = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8).reshape((h, w, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo GIF saved to milling_3d_demo.gif\n"
     ]
    }
   ],
   "source": [
    "create_3d_demo_gif(model, grid_size=8, max_steps=10, output_gif=\"milling_3d_demo.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
